"""
This module takes a json file containing site data and extracts intersection
terms from it. An intersection term in a website is a term that occurs in at
least two of the following locations:

    - url (initial or landing)
    - title
    - text
    - links

These intersections are generated by function intersection_terms(). They can be
futher filtered by removing stopwords and sorted by the number of times they
occur in an area of the website that is visible to the user. The keywords are
generated by function keywords(). There is also a function guess_mld() that
takes the intersection terms and tries to rebuild the main level domain name
using just the keyterms.

This module can also be used via command line as

    $ python keywords.py <path to site data json file>

See the bottom of the file.
"""

# Author:   Kalle Saari kalle.saari@aalto.fi
# Copyright 2015 Secure Systems Group, Aalto University, https://se-sy.org/
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import optparse
import bs4
import collections
import json
import os
import pickle
import publicsuffix
import re
import simple_logger
import sys
from unidecode import unidecode
import urllib

# Public suffix handling
psl = publicsuffix.PublicSuffixList(open('data/public_suffix_list.dat'))

# logging
if __name__ == '__main__':
    logger = simple_logger.SimpleLogger(active=True)
else:
    logger = simple_logger.SimpleLogger(active=False)


####################
# hidden functions #
####################


def _load_json(jspath):
    with open(jspath, 'r') as f:
        js = json.load(f)
    return js


def _get_screenshot_path(jspath):
    js = _get_json(jspath)
    base = os.path.dirname(os.path.dirname(jspath))
    path = os.path.join(base, 'screenshots', js['siteid'] + '.png')
    return path


def _get_ocr(js):
    if 'ocr' in js:
        return js['ocr']
    else:
        logger.print("WARNING: no ocr text found, run ocr!")
        # msg = "WARNING: no ocr text found, run ocr!"
        # warnings.warn(msg, RuntimeWarning)
        return ''


def _replace_ampersands(html):
    """
    Replace ampersand symbols in html.
    """
    html = re.sub('&amp;', '&', html, flags=re.DOTALL)
    html = re.sub('&quot;', '"', html, flags=re.DOTALL)
    # html = re.sub('&amp;', ' ', html, flags=re.DOTALL)
    # html = re.sub('&quot;', ' ', html, flags=re.DOTALL)
    html = re.sub('&lt;', '\<', html)
    html = re.sub('&gt;', '\>', html)
    return html


def _replace_ad(html):
    """
    Remove symbols 'a0:' and 'a2:'. 'a0:' stands for 'nobreak'
    """
    # opening tag
    html = re.sub('\<\s*a\d\:', '<', html)
    # closing tag
    html = re.sub('\</\s*a\d\:', '</', html)
    return html


def _remove_tags(html):
    """
    Remove tags, ie anything like <...>, from input html string.
    """
    html = _replace_ampersands(html)
    html = re.sub('\<sup\>|\</sup\>', '', html)
    html = re.sub('\<sub\>|\</sub\>', '', html)
    tagrx = re.compile('\<.+?\>', flags=re.DOTALL)
    html = tagrx.sub(' ', html)
    return html


def _remove_urls(text):
    """
    Remove urls appearing in text.
    """
    urlrx = re.compile("""http[s]?://[^\s'"]*""")
    text = urlrx.sub(' ', text)
    return text


def _html_to_lines(html, skip_title=False):
    html = _replace_ampersands(html)
    html = _replace_ad(html)
    soup = bs4.BeautifulSoup(html, 'lxml')
    # remove <script> ... </script>
    [x.extract() for x in soup.find_all('script')]
    # remove <noscript> ... </noscript>
    [x.extract() for x in soup.find_all('noscript')]
    # remove <style> ... </style>
    [x.extract() for x in soup.find_all('style')]
    # remove <option>
    [x.extract() for x in soup.find_all('select')]
    # text = soup.get_text(separator=' ', strip=True)
    if skip_title:
        try:
            [x.extract() for x in soup.find('title')]
        except:
            pass
    text = re.sub('\n+', '\n', soup.get_text())
    lines = text.split('\n')
    # # remove non-alphanumerics
    # lines = [re.sub('\W+', ' ', line) for line in lines]
    # # remove numbers
    # lines = [re.sub('\d+', ' ', line) for line in lines]
    # # remove underscores
    # lines = [re.sub('_+', ' ', line) for line in lines]
    # # remove extra white space
    lines = [re.sub('\s+', ' ', line) for line in lines]
    # img tag
    for img in soup.find_all('img'):
        try:
            alt = img['alt']
        except:
            alt = ''
        lines.append(alt)
        try:
            title = img['title']
        except:
            title = ''
        lines.append(title)
    # a tag
    for a in soup.find_all('a'):
        try:
            title = a['title']
        except:
            title = ''
        lines.append(title)
    # input tag
    for inp in soup.find_all('input'):
        try:
            title = inp['title']
        except:
            title = ''
        lines.append(title)
    lines = [_remove_urls(line) for line in lines if line.strip()]
    lines = [line.strip() for line in lines if line.strip()]
    return lines


def _extract_urls(html):
    """
    Try to find all embedded links, whether external or internal
    """
    # substitute real html symbols
    html = _replace_ampersands(html)

    urls = set()

    hrefrx = re.compile("""href\s*\=\s*['"](.*?)['"]""")
    for url in re.findall(hrefrx, html):
        urls.add(str(url))

    srcrx = re.compile("""src\s*\=\s*['"](.*?)['"]""")
    for url in re.findall(srcrx, html):
        urls.add(str(url))

    html = re.sub('%20', ' ', html, flags=re.DOTALL)
    # extract URLs that are not surrounded by quotes
    urlrx = re.compile("""[^'"](http[s]?://[\.a-zA-Z0-9/]+?)\s""")
    for url in re.findall(urlrx, html):
        urls.add(str(url))

    # extract URLs that are surrounded by quotes
    # remove whitespace
    html = re.sub('\s+', '', html)
    urlrx = re.compile("'(http[s]?://[\.a-zA-Z0-9/]+?)'", flags=re.DOTALL)
    urlrx = re.compile('"(http[s]?://[\.a-zA-Z0-9/]+?)"', flags=re.DOTALL)
    for url in re.findall(urlrx, html):
        urls.add(url)

    # remove empty string if exists
    try:
        urls.remove('')
    except KeyError:
        pass

    return sorted(urls)


def _unescape_html(html):
    """
    Replace ampersand symbols in html. NOTE: Python 3.4 has this implemented
    in html module.
    """
    html = re.sub('&amp;', '&', html, flags=re.DOTALL)
    html = re.sub('&quot;', '"', html, flags=re.DOTALL)
    html = re.sub('&lt;', '\<', html)
    html = re.sub('&gt;', '\>', html)
    return html


###############################
# sitedata json manipulations #
###############################


def _js_to_lines(js, skip_title=False):
    html = js['source']
    lines = _html_to_lines(html, skip_title)
    for html in js.get('external_source', {}).values():
        for line in _html_to_lines(html):
            if line not in lines:
                lines.append(line)
    # remove duplicate lines, which might occur if html source are duplicates
    # lines = sorted(set(lines))
    return lines


def _js_to_text(js, skip_title=True):
    text = ' '.join(_js_to_lines(js, skip_title))
    text = unidecode(text)
    return text


def _js_to_urls(js):
    urls = [js['starturl'], js['landurl']]
    sources = [source for source in js.get('external_source', {}).values()]
    sources.append(js['source'])
    html = ' '.join(sources)
    urls += _extract_urls(html)
    # urls += js['loglinks']
    return sorted(set(urls))


def _remove_stopwords(words, list=False, langid='en'):
    """
    Remove English stopwords and frequent www words.
    """
    stopwords = pickle.load(open("data/stopwords_dict", 'br'))
    # stopwords = set(line.strip() for line in open('../data/stopwords_en.txt'))
    stopwords_www = set(line.strip() for line in open('data/stopwords_www.txt'))
    # stopwords = set()
    if list:
        cleaned = []
    else:
        cleaned = set()
    for token in words:
            if token not in stopwords_www and token not in stopwords[langid]:
                if list:
                    cleaned.append(token)
                else:
                    cleaned.add(token)
    return cleaned


def prune_link(link):
    """
    Remove protocol and file extension from link

    Example
    -------
    >>> prune_link("http://www.hs.fi/helsinki.jpg")
    "hs/helsinki"
    >>> prune_link("./image.png")
    'image'
    """
    # FIRST: take care of relative links
    if link.startswith('/'):
        return os.path.splitext(link[1:])[0]
    if link.startswith('./'):
        return os.path.splitext(link[2:])[0]
    if link.startswith('../'):
        return os.path.splitext(link[3:])[0]
    # logger.print("Trying to prune link:", [link], oneline=True)
    # SECOND: treat full urls
    parsed = urllib.parse.urlparse(link)
    netloc = parsed.netloc
    # erase www
    if netloc[:4] == 'www.':
        netloc = netloc[4:]
    # erase tld
    mld, ps = split_mld_ps(netloc)
    try:
        netloc = netloc[:-len(ps) - 1]  # extra -1 covers '.'
    except TypeError:
        netloc = ''
    # remove extension from path
    path = os.path.splitext(parsed.path)[0]
    return netloc + path


def _prune_bifixes(set_of_words):
    """
    Remove words from a set that are proper prefixes or suffixes of another word

    Example
    -------
    >>> set_of_words = {'aba', 'a', 'aaba'}
    >>> _prune_bifixes(set_of_words)
    {'aaba'}
    """
    cleaned = set()
    bad_words = set()
    for elem1 in set_of_words:
        for elem2 in set_of_words:
            if elem1 == elem2:
                continue
            if elem2.startswith(elem1) or elem2.endswith(elem1):
                bad_words.add(elem1)
    cleaned = set.difference(set_of_words, bad_words)
    return cleaned


def _remove_inner_punctuation(string):
    """
    If two strings are separated by & or  -, remove
    the symbol and join the strings.

    Example
    -------
    >>> _remove_inner_punctuation("This is AT&T here")
    'This is ATT here'
    >>> _remove_inner_punctuation("A T & T")
    'A T  T'
    """
    string = re.sub('[\&\-]', '', string)
    return string


def _tokens_in_string(tokens, string, is_url=False):
    """
    Find tokens that occur in string. Return a set of tokens.

    Example
    -------
    >>> _tokens_in_string(['hi', 'ring', 'aa'], 'this is a string', is_url=False)
    {'hi', 'ring'}
    >>> _tokens_in_string(['tt', 'ww', 'wh', 'hs', 'fi', 'sin', 'jpg'], 'http://www.hs.fi/helsinki.jpg', is_url=True)
    {'hs', 'sin'}
    NOTE: 'www' is stripped by prune_link. That's why 'ww' is not in the intersection set
    """
    # tokens = ' '.join(tokens).lower().split()
    # tokens = [w.lower() for w in tokens if w.strip()]
    # Assuming tokens are already lowercased
    # If there are too many tokens, this will be a bottle nect. 
    # Thus we chop the list of tokens, if necessary.
    if len(tokens) > 4000:
        tokens = tokens[:2000] + tokens[-2000:]
    string = string.lower()
    if is_url:
        string = prune_link(string)
    intersection = set()
    for token in tokens:
        if token in string:
            intersection.add(token)
    return intersection


def _add_counts(js={}, intersection=set(), website=None, use_ocr=False):
    """
    For each intersection term, count the number of times it appears in the json file.
    """
    if website is not None:
        js = website.js
    d = {}
    js_dump = js['starturl'].lower() + ' ' + js['landurl'].lower() + ' '
    if use_ocr:
        js_dump += _get_ocr(js).lower()
    else:
        js_dump += _js_to_text(js, skip_title=False).lower()
    # js_dump += ' ' + ' '.join(js['loglinks']).lower()
    js_dump = _unescape_html(js_dump)
    js_dump = _remove_inner_punctuation(js_dump)
    for term in intersection:
        d[term] = js_dump.count(term)
    return d


def _sort_by_count(js={}, intersection=set(), website=None, use_ocr=False):
    """
    Return an intersection set by the number of times tokens appear in json file.
    Most frequent tokens appear first.
    """
    li = sorted(_add_counts(js, intersection, website, use_ocr=use_ocr).items(), key=lambda x: x[1], reverse=True)
    return [x[0] for x in li]


####################
# public functions #
####################


def title_tokens_in_url(js):
    """
    Extract tokens in title string that occur in start or landing url.
    """
    tokenstring = js['title'].lower()
    tokenstring = tokenstring.lower()
    tokenstring = _remove_inner_punctuation(tokenstring)
    # replace digits with space
    tokenstring = re.sub('\d+', ' ', tokenstring)
    # merge words separated by hyphens, e.g., e-mail
    tokenstring = re.sub('\-+', '', tokenstring)
    # replace non-alphanumeric and  non-underscore symbols with space
    tokenstring = re.sub('\W+', ' ', tokenstring)
    # replace underscore with space
    tokenstring = re.sub('_+', ' ', tokenstring)
    # split on to spaces
    tokens = re.split('\s+', tokenstring)
    # reject too short tokens
    tokens = [token for token in tokens if len(token) >= 1]

    intersection = set()
    for string in [js['starturl'], js['landurl']]:
        intersection |= _tokens_in_string(tokens, string, is_url=True)
    logger.print("title tokens in url:", intersection)
    # print(intersection)
    return intersection


def text_tokens_in_url(js, use_ocr=False):
    """
    Extract tokens in text string that occur in start or landing url.
    """
    if use_ocr:
        tokenstring = _get_ocr(js).lower()
    else:
        tokenstring = _js_to_text(js, skip_title=True).lower()
        # tokenstring = js['text'].lower()
        
    tokenstring = _remove_inner_punctuation(tokenstring)

    # replace digits with space
    tokenstring = re.sub('\d+', ' ', tokenstring)
    # merge words separated by hyphens, e.g., e-mail
    tokenstring = re.sub('\-+', '', tokenstring)
    # replace non-alphanumeric and  non-underscore symbols with space
    tokenstring = re.sub('\W+', ' ', tokenstring)
    # replace underscore with space
    tokenstring = re.sub('_+', ' ', tokenstring)
    # split on to spaces
    tokens = re.split('\s+', tokenstring)
    # ignore tokens with less than 3 characters
    tokens = [token for token in tokens if len(token) >= 1]
    # logger.print("text tokens:", tokens[:5])

    intersection = set()
    for string in js['starturl'], js['landurl']:
        intersection |= _tokens_in_string(tokens, string, is_url=True)
    logger.print("text tokens in url:", intersection)
    return intersection


def title_tokens_in_links(js):
    """
    Extract tokens in title string that occur in static or dynamically
    generated links.
    """
    tokenstring = js['title'].lower()
    tokenstring = _remove_inner_punctuation(tokenstring)

    # replace digits with space
    tokenstring = re.sub('\d+', ' ', tokenstring)
    # merge words separated by hyphens, e.g., e-mail
    tokenstring = re.sub('\-+', '', tokenstring)
    # replace non-alphanumeric and  non-underscore symbols with space
    tokenstring = re.sub('\W+', ' ', tokenstring)
    # replace underscore with space
    tokenstring = re.sub('_+', ' ', tokenstring)
    # split on to spaces
    tokens = re.split('\s+', tokenstring)
    # ignore tokens with less than 1 characters
    tokens = [token for token in tokens if len(token) >= 1]

    intersection = set()
    for link in _js_to_urls(js):
        # the following domains are automatically produced by Firefox and thus useless
        if split_mld_ps(link)[0] in set(['mozilla', 'digicert', 'symcd', 'symcb']):
            continue
        intersection |= _tokens_in_string(tokens, link, is_url=True)
    logger.print("title tokens in links:", intersection)
    return intersection


def text_tokens_in_links(js, use_ocr=False):
    """
    Extract tokens in text string that occur in static or dynamically
    generated links.
    """
    if use_ocr:
        tokenstring = _get_ocr(js).lower()
    else:
        tokenstring = _js_to_text(js, skip_title=True).lower()
        # tokenstring = js['text'].lower()
        
    # tokenstring = _js_to_text(self, js, skip_title=True)
    # tokenstring = _unescape_html(tokenstring)
    tokenstring = _remove_inner_punctuation(tokenstring)

    # replace digits with space
    tokenstring = re.sub('\d+', ' ', tokenstring)
    # replace hyphens with space
    tokenstring = re.sub('\-+', ' ', tokenstring)
    # replace non-alphanumeric and  non-underscore symbols with space
    tokenstring = re.sub('\W+', ' ', tokenstring)
    # replace underscore with space
    tokenstring = re.sub('_+', ' ', tokenstring)
    # split on to spaces
    tokens = re.split('\s+', tokenstring)
    # ignore tokens with less than 3 characters
    tokens = [token for token in tokens if len(token) >= 1]

    intersection = set()
    for link in _js_to_urls(js):
        if split_mld_ps(link)[0] in set(['mozilla', 'digicert', 'symcd', 'symcb']):
            continue
        intersection |= _tokens_in_string(tokens, link, is_url=True)
    logger.print("text tokens in links:", intersection)
    return intersection


def text_tokens_in_title(js, use_ocr=False):
    """
    Extract tokens in text string that occur in title.
    """
    if use_ocr:
        tokenstring = _get_ocr(js)
    else:
        tokenstring = _js_to_text(js, skip_title=True)
        # tokenstring = js['text'].lower()

    # tokenstring1 = _js_to_text(self, js, skip_title=True)
    tokenstring1 = js['text'].lower()
    tokenstring2 = js['title'].lower()
    tokenstring1 = _remove_inner_punctuation(tokenstring1)
    tokenstring2 = _remove_inner_punctuation(tokenstring2)
    # replace digits with space
    tokenstring1 = re.sub('\d+', ' ', tokenstring1)
    tokenstring2 = re.sub('\d+', ' ', tokenstring2)
    # merge words separated by hyphens, e.g., e-mail
    tokenstring1 = re.sub('\-+', '', tokenstring1)
    tokenstring2 = re.sub('\-+', '', tokenstring2)
    # replace non-alphanumeric and  non-underscore symbols with space
    tokenstring1 = re.sub('\W+', ' ', tokenstring1)
    tokenstring2 = re.sub('\W+', ' ', tokenstring2)
    # replace underscore with space
    tokenstring1 = re.sub('_+', ' ', tokenstring1)
    tokenstring2 = re.sub('_+', ' ', tokenstring2)
    # split on to spaces
    tokens1 = re.split('\s+', tokenstring1)
    tokens2 = re.split('\s+', tokenstring2)
    # ignore text tokens with less than 3 characters
    tokens1 = [token for token in tokens1 if len(token) >= 1]
    # ignore title tokens with less than 3 characters
    tokens2 = [token for token in tokens2 if len(token) >= 1]

    intersection = set.intersection(set(tokens1), set(tokens2))
    logger.print("text tokens in title:", intersection)

    return intersection



def copyright_tokens_in_text(js):
    """
    Extract tokens that occur both in text and in   a copyright field.
    """
    pass
    tokenstring1 = ''
    tokenstring2 = ''
    for line in re.split('\n+', js['text']):
        if '@' in line or 'Â©' in line:
            tokenstring1 += ' ' + line.lower()
        else:
            tokenstring2 += ' ' + line.lower()

    tokenstring1 = _remove_inner_punctuation(tokenstring1)
    tokenstring2 = _remove_inner_punctuation(tokenstring2)
    # replace digits with space
    tokenstring1 = re.sub('\d+', ' ', tokenstring1)
    tokenstring2 = re.sub('\d+', ' ', tokenstring2)
    # merge words separated by hyphens, e.g., e-mail
    tokenstring1 = re.sub('\-+', '', tokenstring1)
    tokenstring2 = re.sub('\-+', '', tokenstring2)
    # replace non-alphanumeric and  non-underscore symbols with space
    tokenstring1 = re.sub('\W+', ' ', tokenstring1)
    tokenstring2 = re.sub('\W+', ' ', tokenstring2)
    # replace underscore with space
    tokenstring1 = re.sub('_+', ' ', tokenstring1)
    tokenstring2 = re.sub('_+', ' ', tokenstring2)
    # split on to spaces
    tokens1 = re.split('\s+', tokenstring1)
    tokens2 = re.split('\s+', tokenstring2)
    # ignore text tokens with less than 3 characters
    tokens1 = [token for token in tokens1 if len(token) >= 1]
    # ignore title tokens with less than 3 characters
    tokens2 = [token for token in tokens2 if len(token) >= 1]

    intersection = set.intersection(set(tokens1), set(tokens2))
    logger.print("text tokens in title:", intersection)

    return intersection





# def guess_language(js):
#     text = _js_to_text(js, skip_title=False)
#     intersection = intersection_terms(js)
#     # if there are sufficiently many terms, use this for language detection
#     if len(text.split()) >= len(intersection):
#         lang, conf = self.detector.detect_language(text)
#     else:
#         # if not, use intersection terms
#         cleaned = set()
#         # first: remove internet-stopwords
#         for token in intersection:
#             # if token not in stopwords_www:
#                 cleaned.add(token)
#         # second: detect language
#         logger.print("cleaned intersection terms:", cleaned)
#         lang, conf = self.detector.detect_language(cleaned)
#     logger.print("language {} with confidence {}".format(lang, conf))
#     return lang, conf


def url_intersection_terms(js, use_ocr=False):
    """
    Extract intersection terms that occur in url.
    """
    intersection = set()
    # title tokens in url
    terms = title_tokens_in_url(js)
    intersection |= terms
    intersection |= terms
    # text tokens in url
    terms = text_tokens_in_url(js, use_ocr=use_ocr)
    intersection |= terms
    logger.print('type I tokens:', intersection)
    return intersection


def split_mld_ps(url):
    """
    Extract the main level domain and public suffix from url.

    Parameter
    ---------
    url: str
        url of website

    Returns
    -------
    mld: str
        main level domain
    ps: str
        public suffic

    Example
    -------
    >>> split_mld_ps("http://news.bbc.co.uk/today.html")
    ('bbc', 'co.uk')
    >>> split_mld_ps("https://telepremium.net:443/")
    ('telepremium, 'net')
    """
    try:
        parsed = urllib.parse.urlparse(url)
    except:
        logger.print("warning: failed to parse url")
        return '', ''
    # Next covers urls given both in form google.com and http://google.com
    # It also removes possible port number in netloc
    domain = parsed.netloc.split(':')[0] or parsed.path.split(':')[0]
    domain = psl.get_public_suffix(domain)
    tokens = domain.split('.')
    mld = tokens[0]
    ps = '.'.join(tokens[1:])
    return mld, ps


def intersection_terms(js={}, jspath='', use_ocr=False, boost=True):
    """
    Find tokens that occur in at least two of the following locations:
    - starting and landing url
    - title
    - text
    - links

    Parameters
    ----------
    js : dict, optional
        contains site data
    jspath: str, optional
        if given, json object is loaded from this path
    use_ocr : boolean

    Returns
    -------
    intersection: set
        set of all intersection terms
    """
    if jspath:
        js = _load_json(jspath)
    intersection = set()

    # title tokens in url
    terms = title_tokens_in_url(js)
    intersection |= terms
    # text tokens in url
    terms = text_tokens_in_url(js, use_ocr=use_ocr)
    intersection |= terms
    # text tokens in title
    terms = text_tokens_in_title(js, use_ocr=use_ocr)
    intersection |= terms
    # title tokens in links
    terms = title_tokens_in_links(js)
    intersection |= terms
    # copyright tokens elsewhere in text
    terms = copyright_tokens_in_text(js)
    intersection |= terms
    # text tokens in links
    if boost:
        terms = text_tokens_in_links(js, use_ocr=use_ocr)
        intersection |= terms

    return intersection


def guess_mld(js=None, urlstring='', intersection=None):
    """
    Generate guesses for the main level domain name given intersection terms.

    Parameters
    ----------
    js : json object
        contains the site data
    intersection : set, optional
        contains the intersection terms

    Returns
    -------
    mld_guesses: set
        set of strings, the mld guesses
    """
    if not urlstring:
        urlstring = js['starturl'][len('http://'):] + ' ' + js['landurl'][len('http://'):]
    urlstring = urlstring.lower()
    if intersection is None:
        intersection = intersection_terms(js)
    boosted = intersection.copy()  # shallow copy
    old = set()
    # First, build as long url token as possible combining existing tokens,
    # possibly glued together with -, 0, ..., 9
    while boosted != old:
        old = boosted.copy()
        for token1 in old:
            for token2 in old:
                if token1 + token2 in urlstring:
                    boosted.add(token1 + token2)
                if token1 + '-' + token2 in urlstring:
                    boosted.add(token1 + '-' + token2)
                for d in range(9):
                    if token1 + str(d) + token2 in urlstring:
                        boosted.add(token1 + str(d) + token2)
    # Second, remove intermediate tokens constructed in the previous step
    fullones = set()
    for token1 in boosted:
        if token1 in urlstring:
            for token2 in boosted:
                if token1 + token2 in boosted or token2 + token1 in boosted:
                    break
            else:
                fullones.add(token1)
    # Finally, remove possible one-letter domain guesses
    mld_guesses = set([mld for mld in fullones if len(mld) > 1])
    mld_guesses = _prune_bifixes(mld_guesses)
    mld_guesses = set.difference(mld_guesses, intersection)

    # filter out guess that do not appear in the beginning or in the end of tokenized url
    final_guesses = set()
    tokens = re.split('[\.\/]', urlstring)
    for guess in mld_guesses:
        for token in tokens:
            if token.startswith(guess) or token.endswith(guess):
                final_guesses.add(guess)
    return final_guesses


def keywords(js=None, jspath='', intersection=set(), max_count=None, augment=False, use_ocr=False, boost=True, langid='en', len_lb=3, stopwords=False):
    """
    Extract up to max_count keywords. These words are intersection terms sorted
    by the number of occurrence.

    Parameters
    ----------
    js : json object or None, optional
        contains site data
    jspath : str, optional
        if given, json object is loaded from this path
    intersection : set
        if intersection has been computed before, they can be given here to
        save computation time
    max_count : int, optional, default None
        number of keywords extracted. If None, all keywords are returned
    augment : boolean
        if fewer than max_count keywords are found, augment the set with most
        frequent words in text.
    use_ocr : boolean
        replace text extracted from html source with text obtained by doing ocr
        on screenshot
    len_lb : int (default=3)
        filter out terms of length less than len_lb

    Returns
    -------
    keywords: list
        list of keywords
    """
    if jspath:
        js = _load_json(jspath)
    if not intersection:
        intersection = intersection_terms(js, use_ocr=use_ocr, boost=boost)
    if stopwords:
        # remove stopwords
        intersection = _remove_stopwords(intersection, langid=langid)
    # remove words that are prefixes or suffixes of another intersection term
    intersection = _prune_bifixes(intersection) 
    # remove short words
    intersection = set(token for token in intersection if len(token) >= len_lb)
    keywords = _sort_by_count(js, intersection, use_ocr=use_ocr)

    if augment:
        if use_ocr:
            tokenstring = _get_ocr(js).lower()
        else:
            tokenstring = _js_to_text(js, skip_title=False).lower()

        tokenstring = _remove_inner_punctuation(tokenstring)

        # replace digits with space
        tokenstring = re.sub('\d+', ' ', tokenstring)
        # replace hyphens with space
        tokenstring = re.sub('\-+', ' ', tokenstring)
        # replace non-alphanumeric and  non-underscore symbols with space
        tokenstring = re.sub('\W+', ' ', tokenstring)
        # replace underscore with space
        tokenstring = re.sub('_+', ' ', tokenstring)
        # split on to spaces
        tokens = re.split('\s+', tokenstring)
        # ignore tokens with less than 3 characters
        tokens = [token for token in tokens if len(token) >= 3]
        tokens = _remove_stopwords(tokens, list=True, langid=langid)
        tokens = _prune_bifixes(set(tokens)) 
        counter = collections.Counter(tokens)
        most_common = [x[0] for x in counter.most_common()]


    if max_count is None:
        return keywords
    elif augment and len(keywords) < max_count:
        most_common = [token for token in most_common if token not in keywords]
        augmentation = most_common[:max_count - len(keywords)]
        return keywords + augmentation
    else:
        return keywords[:max_count]


########
# main #
########


if __name__ == '__main__':
    usage = "usage:\n\tpython {} [--verbose] {{sequence of sitedata json files}}".format(__file__)

    op = optparse.OptionParser()
    op.add_option('--verbose', action='store_true', dest='verbose', help='activate verbose output') 
    op.add_option('--intersection', action='store_true', dest='intersection', help='print intersection terms')
    (opts, args) = op.parse_args()

    # if not sys.argv[1:]:
    #     print(usage)
    # else:
    if opts.verbose:
        logger.activate()
    else:
        logger.deactivate()
    jsonpaths = args
    for jsonpath in jsonpaths:
        js = json.load(open(jsonpath))
        logger.print("site id: {}".format(js['siteid']))
        logger.print('starting url: {}'.format(js['starturl']))
        logger.print('landing url:  {}'.format(js['landurl']))
        logger.print('title: {}'.format(js['title'][:150]))

        intersection = url_intersection_terms(js)
        intersection = _remove_stopwords(intersection)
        li = _sort_by_count(js, intersection, use_ocr=False)
        logger.print("intersection terms in urls: {}".format(' '.join(li[:10])))

        intersection = intersection_terms(js)
        logger.print("intersection terms:", ' '.join(intersection))
        intersection = _remove_stopwords(intersection)

        logger.print("domain guess:\n{}".format(guess_mld(js, intersection=intersection)))
        logger.print("keywords: {}".format(' '.join(keywords(js))))

        print("real domain:\n{}".format(split_mld_ps(js['landurl'])[0]))
        print("domain guess:\n{}".format(' '.join(guess_mld(js, intersection=intersection))))
        print("keywords:\n{}".format(' '.join(keywords(js))))
